export const PLAIN_SUMMARIES = {"1":"What this article does: Establishes the EU AI Act as a regulation setting harmonised rules for AI systems across the EU, aiming to protect health, safety, and fundamental rights while supporting innovation.\n\nWho it applies to: Everyone in the AI value chain — providers, deployers, importers, distributors, and product manufacturers.\n\nKey requirements:\nPara 1: Sets out the purpose — harmonised rules for placing AI on the market, putting it into service, and using it in the EU.\nPara 2: Lists the Regulation's goals — protect fundamental rights, health and safety, ensure free movement of AI across Member States, prevent restrictions not provided for in this Regulation, and support innovation.","2":"What this article does: Defines who and what falls within the scope of the Regulation, and what is excluded.\n\nWho it applies to: Providers placing AI on the EU market (regardless of where they're established), deployers in the EU, and providers/deployers outside the EU whose AI output is used in the EU.\n\nKey requirements:\nPara 1: Applies to providers placing/putting into service AI in the EU, deployers in the EU, providers and deployers located in third countries where AI output is used in the EU, importers, distributors, product manufacturers, authorised representatives.\nPara 3: Does not apply to AI developed exclusively for military/defence, or to international organisations for law enforcement cooperation.\nPara 4: Does not apply to public authorities in third countries using AI, provided there are adequate data protection safeguards.\nPara 5-6: Excludes AI used purely for scientific R&D, and does not affect rules on liability or intellectual property.\n\nKey exceptions: AI for personal non-professional use is exempt. Military/defence AI is excluded. Scientific R&D AI is excluded unless placed on the market.","3":"What this article does: Provides the 66 official definitions used throughout the EU AI Act.\n\nWho it applies to: All persons interpreting or applying the Regulation.\n\nKey requirements:\nThis article defines all key terms including: 'AI system' (a machine-based system that infers from input to generate outputs), 'provider' (develops or has AI developed and places it on the market), 'deployer' (uses AI under their authority in a professional capacity), 'high-risk AI system', 'biometric data', 'emotion recognition system', 'general-purpose AI model', 'systemic risk', and 63 other terms.\n\nMost critical definitions: 'AI system' (definition 1), 'provider' (definition 3), 'deployer' (definition 4), 'high-risk' determined by Articles 6-7, 'general-purpose AI model' (definition 63).","4":"What this article does: Requires that anyone involved with AI systems ensures adequate AI literacy among their staff.\n\nWho it applies to: Providers and deployers of AI systems.\n\nKey requirements:\nPara 1: Providers and deployers must take measures to ensure their staff and anyone handling AI on their behalf have sufficient AI literacy, considering the persons' technical knowledge, experience, education, context of use, and the people affected by the AI.","5":"What this article does: Bans outright eight categories of AI practices deemed to pose unacceptable risk to fundamental rights, safety, or human dignity. Breaching these prohibitions can attract fines of up to €35 million or 7% of global annual turnover.\n\nWho it applies to: All persons — providers, deployers, and any user of AI in the EU.\n\nKey requirements:\nPara 1(a): Prohibited — AI using subliminal, manipulative, or deceptive techniques that distort behaviour and cause significant harm.\nPara 1(b): Prohibited — AI exploiting vulnerabilities due to age, disability, or social/economic situation.\nPara 1(c): Prohibited — Social scoring systems by public or private actors that lead to detrimental treatment unrelated to the original context.\nPara 1(d): Prohibited — AI assessing individual criminal risk based solely on profiling (not actual criminal behaviour).\nPara 1(e): Prohibited — Untargeted scraping of facial images from the internet or CCTV to build facial recognition databases.\nPara 1(f): Prohibited — Emotion recognition AI in workplace and educational settings (except for medical or safety reasons).\nPara 1(g): Prohibited — Biometric categorisation to infer race, political opinions, trade union membership, religion, or sexual orientation.\nPara 1(h): Prohibited — Real-time remote biometric identification in public spaces for law enforcement, except for narrowly defined exceptions (searching for victims, preventing imminent threats, locating suspects of serious crimes).\n\nKey exceptions: Article 5(h) allows limited law enforcement exceptions for real-time biometric ID in public spaces, subject to prior judicial authorisation and strict necessity/proportionality requirements.","6":"What this article does: Establishes the two-track system for classifying AI as high-risk — either through product safety legislation (Annex I) or through specific use-case categories (Annex III).\n\nWho it applies to: Providers of AI systems who need to determine if their system is high-risk.\n\nKey requirements:\nPara 1: AI embedded in products covered by EU product safety legislation listed in Annex I is high-risk if it must undergo third-party conformity assessment under that legislation.\nPara 2: AI systems listed in Annex III are high-risk.\nPara 3: Exception — an Annex III system is NOT high-risk if it does not pose a significant risk of harm to health, safety, or fundamental rights, because it performs a narrow procedural task, improves the result of a previously completed human activity, detects decision-making patterns without replacing human assessment, or performs a preparatory task.\nPara 4: Providers relying on the paragraph 3 exception must document their assessment and register it in the EU database before placing the system on the market.\nPara 5: The Commission must provide guidelines and examples on applying the paragraph 3 exception.","7":"What this article does: Empowers the Commission to update Annex III (the list of high-risk AI use cases) by adding or modifying entries through delegated acts.\n\nWho it applies to: The European Commission, with implications for all AI providers.\n\nKey requirements:\nPara 1: The Commission may adopt delegated acts to amend Annex III by adding or modifying high-risk AI use-case categories.\nPara 2: When assessing whether to amend, the Commission must consider: the intended purpose, severity and probability of harm, degree of reversibility, extent of adverse impact (especially on vulnerable groups), extent of dependency on the output, and degree of human oversight.\nPara 3: The Commission may also remove entries from Annex III where the conditions in paragraph 2 are no longer met.","8":"What this article does: Requires providers of high-risk AI to comply with the requirements set out in Articles 9-15 (risk management, data governance, documentation, etc.).\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: High-risk AI systems must comply with Articles 9-15, taking into account their intended purpose and the generally acknowledged state of the art.\nPara 2: The intended purpose and risk management system must be considered when ensuring compliance.","9":"What this article does: Mandates that providers of high-risk AI establish, implement, and maintain a continuous risk management system throughout the AI system's lifecycle.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must establish a risk management system — a continuous, iterative process running throughout the AI system's entire lifecycle, requiring regular systematic review and updating.\nPara 2: The risk management system must include: identification and analysis of known and foreseeable risks, estimation and evaluation of risks from intended use and reasonably foreseeable misuse, evaluation of risks arising from post-market monitoring data, and adoption of appropriate risk management measures.\nPara 3: Risk management measures must consider the combined effects of requirements in this Section and reflect the generally acknowledged state of the art.\nPara 4: Testing must be performed to identify the most appropriate risk management measures, including testing under real-world conditions.\nPara 5: Residual risk must be communicated to deployers.\nPara 6: Testing must be suitable for the intended purpose — not exceeding what is necessary, but performed against prior defined metrics and probabilistic thresholds.\nPara 7: The risk management process must consider whether the system is likely to be used by or affect children.","10":"What this article does: Sets requirements for training, validation, and testing data used for high-risk AI systems.\n\nWho it applies to: Providers of high-risk AI systems that use data-driven training techniques.\n\nKey requirements:\nPara 1: Training, validation, and testing data must be subject to appropriate data governance and management practices.\nPara 2: Those practices must address: design choices, data collection and origin, pre-processing operations (annotation, labelling, cleaning, enrichment, aggregation), formulation of relevant assumptions, assessment of data availability/quantity/suitability, examination for possible biases, identification of data gaps or shortcomings, and appropriate measures to address them.\nPara 3: Data must be relevant, sufficiently representative, and as free of errors as possible, with appropriate statistical properties for the intended geographic, behavioural, or functional setting.\nPara 4: Data sets must consider the characteristics of the specific context of deployment.\nPara 5: Processing of special categories of personal data (Article 9 GDPR) is permitted to the extent strictly necessary for bias detection and correction, subject to appropriate safeguards.\nPara 6: For systems not using data-driven training, paragraphs 2-5 apply only to testing data.","11":"What this article does: Requires providers of high-risk AI systems to draw up and maintain technical documentation before the system is placed on the market.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Technical documentation must be drawn up before the system is placed on the market or put into service, and kept up to date. Must demonstrate compliance with the requirements in this Section. SMEs (including start-ups) may provide simplified elements of the technical documentation.\nPara 2: Where a high-risk AI system is related to a product covered by Annex I product safety legislation, a single set of technical documentation covering both is sufficient.\nPara 3: The Commission may update the documentation requirements by delegated act.","12":"What this article does: Requires high-risk AI systems to have automatic logging capabilities to ensure traceability.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: High-risk AI systems must be designed to automatically record events ('logs') while the system is operating, to the extent appropriate to the intended purpose.\nPara 2: Logging must enable monitoring of the system's operation and facilitate post-market monitoring. Must include recording of periods of use, reference database used, input data for which the search led to a match, and identification of the natural persons involved in verifying the results.","13":"What this article does: Requires providers to ensure high-risk AI systems are designed to be transparent and to provide deployers with adequate instructions for use.\n\nWho it applies to: Providers (to design for transparency) and deployers (to receive and use the instructions).\n\nKey requirements:\nPara 1: High-risk AI systems must be designed and developed to ensure their operation is sufficiently transparent to enable deployers to interpret and use the output appropriately.\nPara 2: Systems must be accompanied by instructions for use in a suitable digital format, including: identity and contact details of the provider, system characteristics/capabilities/limitations, intended purpose, level of accuracy/robustness/cybersecurity, known circumstances of foreseeable misuse, installation and maintenance instructions, description of logs, and human oversight measures.\nPara 3: Instructions must be concise, correct, clear, relevant, accessible, and comprehensible to deployers.","14":"What this article does: Requires high-risk AI systems to be designed to allow effective human oversight during use.\n\nWho it applies to: Providers (design for oversight) and deployers (implement oversight measures).\n\nKey requirements:\nPara 1: Systems must be designed to enable human oversight during use, including by means of human-machine interface tools.\nPara 2: Oversight must aim to prevent or minimise risks to health, safety, or fundamental rights, particularly where such risks may emerge despite other requirements being met.\nPara 3: Oversight measures must be proportionate and may include: the ability for a human to fully understand the system's capacities and limitations, to correctly interpret output, to decide not to use the system or to disregard/reverse/override output, and to intervene or halt the system.\nPara 4: For real-time remote biometric identification, there must always be a human who can decide not to act on the AI's identification, and the AI must not autonomously trigger action.\nPara 5: Oversight must ensure no action or decision is taken based solely on the AI's identification without independent verification by at least two natural persons.","15":"What this article does: Requires high-risk AI systems to achieve appropriate levels of accuracy, robustness, and cybersecurity.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Systems must achieve appropriate levels of accuracy, robustness, and cybersecurity and perform consistently throughout their lifecycle.\nPara 2: Accuracy levels and metrics must be declared in the instructions for use.\nPara 3: Systems must be resilient to errors, faults, or inconsistencies — both within the system and in its operating environment.\nPara 4: Technical redundancy solutions (backup, fail-safe, etc.) must be appropriate.\nPara 5: Systems must be resilient against attempts by unauthorised third parties to exploit vulnerabilities (e.g., data poisoning, adversarial examples, model manipulation, confidentiality attacks).","16":"What this article does: Sets out the core obligations on providers of high-risk AI systems.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara (a)-(m): Providers must: ensure compliance with requirements in Articles 8-15, have a quality management system (Article 17), keep documentation (Article 18), keep logs where under their control (Article 19), ensure conformity assessment before placing on market (Article 43), register in the EU database (Article 49/71), take corrective action for non-conformity (Article 20), inform competent authorities of risks (Article 20), apply CE marking (Article 48), and demonstrate compliance upon request.","17":"What this article does: Requires providers of high-risk AI systems to establish a quality management system.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must put in place a quality management system ensuring compliance, documented in a systematic and orderly manner as written policies, procedures, and instructions.\nPara 2: The QMS must include: compliance strategy, design and development techniques, quality control procedures, examination/test/validation procedures (pre/during/post development), technical specifications and standards applied, data management systems, risk management system, post-market monitoring system, incident reporting procedures, communication procedures with competent authorities and deployers, and record-keeping policies.\nPara 3: For SMEs, requirements must be proportionate to their size.","18":"What this article does: Requires providers to keep technical documentation and related records.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must keep technical documentation referred to in Article 11 for 10 years after the system is placed on the market.\nPara 2: Also keep: documentation on the quality management system, decisions and actions by notified bodies, EU declaration of conformity, and any changes approved by notified bodies.\nPara 3: When the provider is a public body, documentation obligations apply to the deploying authority.","19":"What this article does: Requires providers to retain automatically generated logs for high-risk AI systems.\n\nWho it applies to: Providers of high-risk AI systems (and deployers where logs are under their control).\n\nKey requirements:\nPara 1: Providers must keep the logs automatically generated by their high-risk AI system for at least 6 months, unless otherwise provided by EU or national law.","20":"What this article does: Requires providers to take corrective action for non-conforming AI and to inform authorities.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers who consider (or have reason to consider) that their high-risk AI system is not in conformity must immediately take necessary corrective actions — including withdrawal, disabling, or recall.\nPara 2: Must immediately inform distributors, deployers, authorised representatives, and importers.\nPara 3: Where the system presents a risk to health/safety/fundamental rights, must immediately inform the market surveillance authority of the Member State where the system was made available, plus the notified body (if any), giving full details of the non-conformity and corrective actions taken.","21":"What this article does: Requires providers of high-risk AI systems to cooperate with competent authorities.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must, upon a reasoned request from a competent authority, provide all information and documentation necessary to demonstrate conformity, in the official language of the relevant Member State.\nPara 2: Must grant the authority access to the automatically generated logs (where under the provider's control) to the extent relevant for the request.","22":"What this article does: Allows providers of high-risk AI established outside the EU to appoint an authorised representative.\n\nWho it applies to: Non-EU providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers established in third countries must, by written mandate, appoint an authorised representative established in the EU prior to placing their system on the EU market.\nPara 2: The mandate must authorise the representative to: verify the declaration of conformity and technical documentation, provide competent authorities with all information necessary, cooperate with competent authorities on corrective actions, and inform the provider of complaints and risks.\nPara 3: The mandate must not delegate compliance with core provider obligations (Articles 9-17).\nPara 4: The representative can terminate the mandate with reasonable notice if the provider acts contrary to its obligations.","23":"What this article does: Sets out obligations for importers of high-risk AI systems into the EU.\n\nWho it applies to: Importers of high-risk AI systems.\n\nKey requirements:\nPara 1: Importers must ensure the system has undergone conformity assessment, the provider has drawn up technical documentation, the system bears CE marking, and is accompanied by the required documentation and instructions for use.\nPara 2: Where an importer considers a system is non-conforming, they must not place it on the market until it has been brought into conformity, and must inform the provider and market surveillance authorities.\nPara 3: Must indicate their name, trade name, and contact address on the system or its documentation.\nPara 4: Must ensure appropriate storage and transport conditions.\nPara 5: Must keep a copy of the EU declaration of conformity and technical documentation for 10 years.\nPara 6: Must provide competent authorities with all necessary information and documentation upon reasoned request.","24":"What this article does: Sets out obligations for distributors of high-risk AI systems.\n\nWho it applies to: Distributors of high-risk AI systems.\n\nKey requirements:\nPara 1: Before making a high-risk AI system available on the market, distributors must verify that it bears the CE marking, is accompanied by the required documentation, and that the provider and importer have complied with their obligations.\nPara 2: Where a distributor considers a system is non-conforming, they must not make it available on the market until brought into conformity, and must inform the provider or importer and market surveillance authorities.\nPara 3: Must ensure appropriate storage and transport conditions.\nPara 4: Must provide competent authorities with all necessary information upon request, and cooperate on corrective actions.\nPara 5: Must keep a copy of the EU declaration of conformity for 10 years.","25":"What this article does: Defines when a distributor, importer, deployer, or other third party is treated as if they were the provider (i.e., assumes provider obligations).\n\nWho it applies to: Any economic operator who may become a 'deemed provider' through their actions.\n\nKey requirements:\nPara 1: A distributor, importer, deployer, or third party is considered a provider (and takes on all provider obligations) if they: put their own name or trademark on a high-risk AI system already on the market, make a substantial modification to a high-risk AI system already on the market, or modify the intended purpose of an AI system (including a general-purpose one) in a way that makes it high-risk.\nPara 2: Where this happens, the original provider is no longer considered the provider for the modified system, but must cooperate by sharing necessary information and providing reasonably expected technical access.\nPara 3: For AI systems that are safety components: the product manufacturer bears the same obligations as the provider.\nPara 4: Downstream operators who make substantial modifications must ensure the system still complies and must cooperate with post-market monitoring.","26":"What this article does: Sets out the comprehensive obligations for deployers (users) of high-risk AI systems.\n\nWho it applies to: Deployers of high-risk AI systems (any organisation using such systems professionally).\n\nKey requirements:\nPara 1: Deployers must take appropriate technical and organisational measures to ensure they use high-risk AI in accordance with the instructions for use.\nPara 2: Must assign human oversight to natural persons who have the necessary competence, training, authority, and support.\nPara 3: Must ensure input data is relevant and sufficiently representative for the intended purpose.\nPara 4: Must monitor operation based on the instructions for use. Where there are reasons to consider a risk to health/safety/fundamental rights, must immediately inform the provider or distributor and suspend use.\nPara 5: Must keep automatically generated logs for at least 6 months.\nPara 6: Must use information from the instructions for use to carry out a data protection impact assessment (DPIA) where required under GDPR.\nPara 7: Must cooperate with competent authorities.\nPara 8: Public bodies (or private entities providing public services) using high-risk AI for decisions about natural persons must not rely solely on the AI output, must verify and correct output, and must inform natural persons that AI was used in the decision.\nPara 9: Deployers of emotion recognition or biometric categorisation systems must inform natural persons exposed to the system.\nPara 10: Deployers of high-risk AI that generates or manipulates image, audio, or video content must disclose that the content is AI-generated.","27":"What this article does: Requires deployers of high-risk AI used in certain public-facing contexts to carry out a fundamental rights impact assessment before first use.\n\nWho it applies to: Deployers that are public bodies or private entities providing public services, and deployers of AI systems listed in certain Annex III categories.\n\nKey requirements:\nPara 1: Applicable deployers must perform a fundamental rights impact assessment before putting the system into use.\nPara 2: The assessment must include: a description of the deployer's processes where the AI will be used, a description of the period of and frequency of use, the categories of natural persons and groups likely to be affected, the specific risks of harm likely to affect those persons/groups, a description of human oversight measures, the measures to be taken if those risks materialise, and the governance and complaint mechanisms.\nPara 3: Where the deployer already has a DPIA under GDPR, the fundamental rights impact assessment may complement it.\nPara 4: The assessment must be notified to the market surveillance authority.\nPara 5: Must be updated when conditions change substantially.","28":"What this article does: Requires Member States to designate notifying authorities responsible for assessing and notifying conformity assessment bodies.\n\nWho it applies to: EU Member States.\n\nKey requirements:\nPara 1: Each Member State must designate or establish at least one notifying authority responsible for setting up and carrying out the procedures for assessing and notifying conformity assessment bodies, and for monitoring notified bodies.\nPara 2: The notifying authority must be established to avoid conflicts of interest with conformity assessment bodies.\nPara 3: Must have a sufficient number of competent personnel.\nPara 4: Must have adequate financial resources.","29":"What this article does: Sets out the process for conformity assessment bodies to apply for notification (becoming 'notified bodies').\n\nWho it applies to: Conformity assessment bodies seeking to become notified bodies under the EU AI Act.\n\nKey requirements:\nPara 1: Bodies must submit an application to the notifying authority of the Member State where they are established.\nPara 2: The application must include a description of conformity assessment activities, modules, and types of AI systems, plus a certificate of accreditation confirming the body meets the requirements.\nPara 3: Where no accreditation certificate can be provided, the body must provide the notifying authority with all documentary evidence necessary to verify compliance.","30":"What this article does: Establishes the procedure for notifying conformity assessment bodies to the Commission and other Member States.\n\nWho it applies to: Notifying authorities of Member States.\n\nKey requirements:\nPara 1: Notifying authorities may only notify bodies that satisfy the requirements in Article 31.\nPara 2: Must notify the Commission and other Member States using the electronic notification tool developed by the Commission.\nPara 3: The notification must include full details of the conformity assessment activities, modules, types of AI systems, and relevant attestation of competence.\nPara 4: The body may only perform notified body activities if no objections are raised by the Commission or other Member States within one month of notification.\nPara 5: The notifying authority must inform the Commission and other Member States of any subsequent relevant changes.","31":"What this article does: Sets out the requirements that conformity assessment bodies must meet to be designated as notified bodies.\n\nWho it applies to: Conformity assessment bodies seeking notification, and notifying authorities assessing them.\n\nKey requirements:\nPara 1: Must be established under national law and have legal personality.\nPara 2: Must be independent of the AI system provider and any other operator with an economic interest.\nPara 3: Must be organised and operate to safeguard objectivity and impartiality.\nPara 4: Must have documented procedures for performing conformity assessments.\nPara 5-7: Must have sufficient competent personnel with appropriate qualifications, trained in AI technologies. Must have clear responsibilities and reporting structures.\nPara 8: Must carry adequate professional indemnity insurance.\nPara 9: Must comply with confidentiality requirements.\nPara 10: Must participate in coordination and harmonisation activities.\nPara 11: Must demonstrate knowledge of and ability to apply harmonised standards and common specifications.","32":"What this article does: Creates a presumption of conformity with the notified body requirements for bodies that have been certified under harmonised standards.\n\nWho it applies to: Conformity assessment bodies.\n\nKey requirements:\nPara 1: Where a conformity assessment body demonstrates conformity with harmonised standards (or parts thereof), it shall be presumed to comply with the requirements in Article 31 to the extent those standards cover those requirements.","33":"What this article does: Allows notified bodies to use subsidiaries and subcontractors but with safeguards.\n\nWho it applies to: Notified bodies.\n\nKey requirements:\nPara 1: Where a notified body subcontracts specific tasks or uses a subsidiary, it must ensure the subcontractor/subsidiary meets the requirements in Article 31 and must inform the notifying authority.\nPara 2: The notified body remains fully responsible for the tasks performed by subcontractors or subsidiaries.\nPara 3: Conformity assessment activities may only be subcontracted or performed by a subsidiary with the agreement of the provider.","34":"What this article does: Sets operational obligations for notified bodies when carrying out conformity assessments.\n\nWho it applies to: Notified bodies.\n\nKey requirements:\nPara 1: Notified bodies must verify the conformity of high-risk AI systems in accordance with the conformity assessment procedures in Article 43.\nPara 2: Must avoid unnecessary burdens on providers and act proportionately to the size and structure of the business.\nPara 3: Where a notified body finds that a provider has not met the requirements, it must require the provider to take appropriate corrective measures and must not issue a certificate of conformity.","35":"What this article does: Requires the Commission to manage identification numbers and public lists of notified bodies.\n\nWho it applies to: The Commission and notified bodies.\n\nKey requirements:\nPara 1: The Commission must assign identification numbers to notified bodies.\nPara 2: The Commission must make the list of notified bodies publicly available, including their identification numbers and the activities for which they have been notified.","36":"What this article does: Establishes procedures for changes to notifications, suspension, and restriction of notified bodies.\n\nWho it applies to: Notifying authorities, notified bodies, and the Commission.\n\nKey requirements:\nPara 1-2: Where a notifying authority finds a notified body no longer meets Article 31 requirements, or is failing to fulfil its obligations, it must restrict, suspend, or withdraw the notification as appropriate.\nPara 3-4: Must immediately inform the Commission and other Member States of any restriction, suspension, or withdrawal.\nPara 5-8: The Commission must ensure affected providers are informed and can seek alternative notified bodies. Certificates issued by the affected body remain valid during transition.\nPara 9-10: The notifying authority must investigate and may reinstate the notification once the issues are resolved.\nPara 11: Where a national authority disputes a notified body's competence, it must inform the Commission, which shall assess and may request the Member State to take corrective action.","37":"What this article does: Allows the Commission to investigate and challenge the competence of notified bodies.\n\nWho it applies to: The Commission, notifying authorities, and notified bodies.\n\nKey requirements:\nPara 1: The Commission may investigate where there are doubts about a notified body's competence or ongoing compliance.\nPara 2: The Commission must provide the notifying authority with all relevant information.\nPara 3: The notifying authority must take all appropriate measures, which may include restricting, suspending, or withdrawing the notification.","38":"What this article does: Requires coordination among notified bodies under the EU AI Act.\n\nWho it applies to: Notified bodies and the Commission.\n\nKey requirements:\nPara 1: The Commission must ensure appropriate coordination and cooperation between notified bodies active under this Regulation, organised as a sectoral group of notified bodies.","39":"What this article does: Allows conformity assessment bodies established in third countries to apply to become notified bodies under certain conditions.\n\nWho it applies to: Third-country conformity assessment bodies.\n\nKey requirements:\nPara 1: Bodies established in a third country may be accepted as notified bodies where authorised under an international agreement or where they can demonstrate equivalent compliance with Article 31 requirements.","40":"What this article does: Establishes the role of harmonised standards in the EU AI Act compliance framework.\n\nWho it applies to: Providers of high-risk AI systems and standards organisations.\n\nKey requirements:\nPara 1: High-risk AI systems that conform to harmonised standards (or relevant parts) published in the Official Journal shall be presumed to comply with the requirements in Articles 8-15 to the extent those standards cover those requirements.\nPara 2: The Commission must request European standardisation organisations to deliver harmonised standards covering the requirements.\nPara 3: The Commission may adopt implementing acts specifying common specifications where no harmonised standards exist or where they are insufficient.\nPara 4: Providers not following harmonised standards must demonstrate their alternative approach achieves at least an equivalent level of protection.","41":"What this article does: Allows the Commission to adopt common specifications where harmonised standards don't exist or are insufficient.\n\nWho it applies to: The Commission and providers of high-risk AI systems.\n\nKey requirements:\nPara 1: The Commission may adopt implementing acts establishing common specifications for the requirements in Articles 8-15 where no harmonised standards exist or where existing ones are insufficient.\nPara 2: Must consult the AI Board and relevant stakeholders before adoption.\nPara 3: Where common specifications are not fully followed, providers must demonstrate their alternative approach achieves at least an equivalent level of protection.\nPara 4: Common specifications must be reviewed regularly.","42":"What this article does: Creates a presumption of conformity with certain requirements for AI systems that comply with harmonised standards or common specifications.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: High-risk AI systems trained and tested on data reflecting the specific EU geographic, behavioural, or functional setting shall be presumed to comply with Article 10(4) requirements (deployment-specific data suitability).\nPara 2: Systems conforming to harmonised standards or common specifications on accuracy, robustness, and cybersecurity are presumed to comply with Article 15.","43":"What this article does: Establishes which conformity assessment procedure applies to high-risk AI systems, determining whether self-assessment or third-party assessment by a notified body is required.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: For systems listed in Annex I (product safety legislation), the conformity assessment follows the procedure already required under that legislation. The requirements of Articles 8-15 must also be assessed as part of that procedure.\nPara 2: For AI systems referred to in Annex III point 1 (biometric systems), a third-party conformity assessment via a notified body is required.\nPara 3: For other Annex III high-risk AI systems, providers may use the internal control procedure (self-assessment) in Annex VI.\nPara 4: Where the provider has applied harmonised standards or common specifications, and where the notified body was involved in assessing the quality management system, the certificate covers both the quality management system and the technical documentation.\nPara 5: Regardless of whether a notified body was involved, the EU declaration of conformity must be issued before the system is placed on the market.\nPara 6: The Commission may adopt delegated acts to amend which Annex III systems require third-party assessment, considering the maturity of the market and the availability of harmonised standards.\n\nKey exceptions: Most Annex III systems (except biometric ones) can use self-assessment. Annex I product systems follow their existing sectoral conformity assessment path.","44":"What this article does: Sets rules for the certificates issued by notified bodies.\n\nWho it applies to: Notified bodies and providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Certificates issued by notified bodies must be in the official language of the Member State and valid for the period indicated (maximum 5 years).\nPara 2: Where a notified body finds the system no longer meets requirements, it must suspend or withdraw the certificate.\nPara 3: The Commission may adopt implementing acts on the form and content of certificates.","45":"What this article does: Requires notified bodies to share information with authorities, other notified bodies, and the Commission.\n\nWho it applies to: Notified bodies.\n\nKey requirements:\nPara 1: Must inform the notifying authority of any refused, restricted, suspended, or withdrawn certificates.\nPara 2: Must inform the Commission and other Member States of certificates refused, suspended, restricted, or withdrawn, and of certificates issued.\nPara 3: Must provide other notified bodies performing similar activities with relevant information on negative and positive conformity assessment results.\nPara 4: Must retain certificates and documentation for the Commission, Board, and other notified bodies.","46":"What this article does: Allows derogation from the standard conformity assessment procedure in exceptional circumstances.\n\nWho it applies to: Market surveillance authorities and providers.\n\nKey requirements:\nPara 1: Any market surveillance authority may authorise the placing on the market of a specific high-risk AI system without the standard conformity assessment, on the basis of exceptional reasons of public security or protection of life and health, environmental protection, or protection of key industrial and infrastructural assets.\nPara 2: Such authorisation must be for a limited period and may be subject to conditions.\nPara 3: The Commission must be informed and may extend or revoke the authorisation.","47":"What this article does: Requires providers to draw up an EU declaration of conformity for each high-risk AI system.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must draw up a written machine-readable, physical, or electronically signed EU declaration of conformity for each high-risk AI system.\nPara 2: The declaration must state the AI system is in conformity with the requirements of Articles 8-15.\nPara 3: Must be kept for 10 years after the system is placed on the market.\nPara 4: The declaration must contain the provider's name and address, a statement of responsibility, identification of the system, the conformity assessment procedure followed, reference to standards or specifications, and the name of the notified body (if applicable).\nPara 5: The Commission may update the content of the declaration by delegated act.","48":"What this article does: Sets rules for affixing the CE marking to high-risk AI systems.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: The CE marking must be affixed visibly, legibly, and indelibly to the high-risk AI system. Where not possible due to the system's nature, it must be affixed to the packaging or documentation.\nPara 2: The CE marking is subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.\nPara 3: Where applicable, the CE marking must be followed by the identification number of the notified body responsible for conformity assessment.\nPara 4: Member States must ensure the CE marking regime is properly implemented and take appropriate action in the event of improper use.","49":"What this article does: Establishes the EU database registration requirements and the information to be provided by providers of high-risk AI systems.\n\nWho it applies to: Providers and deployers of high-risk AI systems listed in Annex III.\n\nKey requirements:\nPara 1: Before placing on the market or putting into service a high-risk AI system listed in Annex III, the provider must register the system and themselves in the EU database.\nPara 2: Deployers that are public authorities (or act on their behalf) must also register in the EU database.\nPara 3: The information provided must be entered using the data formats established by the Commission and must be complete and correct.\nPara 4: The Commission must provide technical and administrative support and publish guidelines for providers and deployers.\n\nKey exceptions: High-risk AI systems related to products covered by Annex I product safety legislation do not need separate database registration under this article.","50":"What this article does: Sets transparency obligations for AI systems that interact with people, generate content, or process biometric data — even if not classified as high-risk.\n\nWho it applies to: Providers and deployers of certain AI systems (regardless of risk classification).\n\nKey requirements:\nPara 1: Providers of AI systems intended to interact directly with natural persons must design the system so that the person is informed they are interacting with an AI (unless this is obvious from the circumstances).\nPara 2: Providers of AI systems that generate synthetic audio, image, video, or text must ensure outputs are marked in a machine-readable format as artificially generated or manipulated.\nPara 3: Deployers of emotion recognition or biometric categorisation systems must inform natural persons exposed to the system.\nPara 4: Deployers of AI systems that generate or manipulate image, audio, or video content constituting a 'deep fake' must disclose that the content is AI-generated.\n\nKey exceptions: AI systems authorised by law for detection, prevention, or investigation of criminal offences may be exempt from some of these transparency requirements. Disclosure obligations for deep fakes do not apply to content that is clearly artistic, creative, satirical, fictional, or analogous.","51":"What this article does: Defines when a general-purpose AI (GPAI) model is classified as having 'systemic risk'.\n\nWho it applies to: Providers of general-purpose AI models.\n\nKey requirements:\nPara 1: A GPAI model is classified as having systemic risk if it has 'high impact capabilities' — determined by appropriate technical tools and methodologies, including benchmarks and indicators.\nPara 2: A GPAI model is presumed to have systemic risk when the cumulative amount of compute used for its training exceeds 10^25 FLOPs.\nPara 3: The Commission may designate additional GPAI models as having systemic risk based on criteria in Annex XIII.","52":"What this article does: Establishes the procedural rules for classifying GPAI models as having systemic risk.\n\nWho it applies to: Providers of general-purpose AI models and the Commission.\n\nKey requirements:\nPara 1: Providers must notify the Commission when their GPAI model meets the criteria for systemic risk classification, within 2 weeks of meeting the criteria.\nPara 2: The Commission may designate a GPAI model as having systemic risk on its own initiative, based on information from the scientific panel or on the basis of criteria in Annex XIII.\nPara 3: A provider that considers its model no longer meets the criteria may request the Commission to reassess the classification.\nPara 4: The Commission must publish a list of GPAI models with systemic risk.","53":"What this article does: Sets the core obligations for providers of all general-purpose AI (GPAI) models.\n\nWho it applies to: Providers of general-purpose AI models.\n\nKey requirements:\nPara 1(a): Must draw up and keep up to date technical documentation of the model, including training and testing process and results, which must be made available to the AI Office and national competent authorities on request.\nPara 1(b): Must draw up and keep up to date information and documentation for downstream providers who integrate the GPAI model into their AI systems, enabling them to understand and comply with their own obligations.\nPara 1(c): Must put in place a policy to comply with EU copyright law, in particular to identify and comply with opt-out reservations expressed by rights holders under Article 4(3) of the DSM Directive.\nPara 1(d): Must draw up and make publicly available a sufficiently detailed summary about the training data, following a template provided by the AI Office.\nPara 2: Obligations in paragraph 1(a) and (b) do not apply to providers of GPAI models released under free and open-source licences, unless they are classified as having systemic risk.\nPara 3: Providers may rely on codes of practice to demonstrate compliance.\nPara 4: Providers that are not subject to systemic risk obligations are only required to keep documentation available for 10 years.\n\nKey exceptions: Open-source GPAI models are exempt from documentation obligations in paragraphs 1(a) and (b), unless they have systemic risk. The copyright policy obligation applies to all GPAI models regardless.","54":"What this article does: Requires non-EU providers of GPAI models to appoint an authorised representative in the EU.\n\nWho it applies to: Providers of GPAI models established outside the EU.\n\nKey requirements:\nPara 1: Non-EU providers must appoint an authorised representative established in the EU before making their model available on the EU market.\nPara 2: The mandate must authorise the representative to: perform all relevant tasks on behalf of the provider, cooperate with the AI Office and competent authorities, and provide documentation upon request.\nPara 3: The representative can terminate the mandate if the provider acts contrary to its obligations.","55":"What this article does: Imposes additional obligations on providers of GPAI models classified as having systemic risk.\n\nWho it applies to: Providers of GPAI models with systemic risk.\n\nKey requirements:\nPara 1(a): Must perform model evaluation, including conducting and documenting adversarial testing, to identify and mitigate systemic risk.\nPara 1(b): Must assess and mitigate possible systemic risks, including their sources.\nPara 1(c): Must track, document, and report serious incidents and possible corrective measures to the AI Office and relevant national competent authorities without undue delay.\nPara 1(d): Must ensure an adequate level of cybersecurity protection for the model and its physical infrastructure.\nPara 2: May rely on codes of practice to demonstrate compliance. Where no code of practice is available, must comply with alternative measures approved by the Commission.\nPara 3: Must cooperate with the AI Office and provide it with any necessary information.","56":"What this article does: Establishes the framework for codes of practice for GPAI model providers.\n\nWho it applies to: Providers of GPAI models and the AI Office.\n\nKey requirements:\nPara 1: The AI Office must encourage and facilitate the drawing up of codes of practice at EU level, taking into account international approaches.\nPara 2: Codes must cover obligations in Articles 53 and 55, including copyright compliance, training data summaries, and systemic risk mitigation.\nPara 3: All GPAI providers and relevant stakeholders, including civil society and academia, should be able to participate in drafting.\nPara 4: Codes must be ready by the application date of the GPAI obligations.\nPara 5: The AI Office may invite all providers to adhere to the codes.\nPara 6: Must be reviewed and updated regularly.\nPara 7-9: Where no satisfactory code of practice emerges, the Commission may adopt implementing acts providing common rules.","57":"What this article does: Establishes the framework for AI regulatory sandboxes — controlled environments for testing innovative AI systems under regulatory supervision.\n\nWho it applies to: National competent authorities, the AI Office, and any persons wishing to develop, test, or validate AI systems in a sandbox.\n\nKey requirements:\nPara 1: Member States must ensure their competent authorities establish at least one AI regulatory sandbox at national level, operational by the application date. Sandboxes may also be established at regional or local level, or jointly with other Member States.\nPara 2: Additional sandboxes may be established at EU level by the Commission and the AI Office.\nPara 3: Sandboxes must offer a controlled environment for development, testing, and validation of innovative AI systems under the direct supervision of competent authorities, before placement on the market.\nPara 4: Competent authorities must provide guidance, supervision, and support within the sandbox. They must identify risks to health, safety, and fundamental rights and agree on an exit plan.\nPara 5: Participation in the sandbox does not exempt participants from liability for harm caused.\nPara 6: Personal data collected outside the sandbox may be processed within it only under specific conditions.\nPara 7: Member States must establish governance and accountability frameworks for sandboxes.\nPara 8-15: Detailed rules on data protection safeguards, reporting, scope of testing, and cross-border cooperation.","58":"What this article does: Sets out detailed rules for the practical operation and functioning of AI regulatory sandboxes.\n\nWho it applies to: Competent authorities operating sandboxes and participants.\n\nKey requirements:\nPara 1: National competent authorities must publish the conditions for participation, selection criteria, application process, and duration. Criteria must be fair, transparent, and non-discriminatory.\nPara 2: Must provide information on rights and safeguards for affected persons.\nPara 3: Sandbox participants retain full liability for any harm caused.\nPara 4: Competent authorities must exercise their supervisory and corrective powers within the sandbox.\nPara 5: Results and learnings from sandboxes must be documented and shared with the AI Board and the Commission.","59":"What this article does: Allows the further processing of personal data within AI regulatory sandboxes for the purpose of developing AI systems in the public interest, subject to strict safeguards.\n\nWho it applies to: Sandbox participants developing AI systems in the public interest (e.g., public safety, health, environment).\n\nKey requirements:\nPara 1: Personal data lawfully collected for other purposes may be processed in the sandbox for developing, training, and testing AI systems, subject to all conditions in this article.\nPara 2: Conditions include: the AI system must be developed for safeguarding substantial public interest, the data processed must be necessary for compliance with one or more requirements in Articles 8-15, there must be effective monitoring mechanisms, the personal data must be protected by appropriate technical and organisational measures, the data must not be transmitted to others, and the data must be deleted when participation in the sandbox ends.\nPara 3: This article is without prejudice to national law authorising processing of personal data.","60":"What this article does: Sets rules for testing high-risk AI systems in real-world conditions outside of regulatory sandboxes.\n\nWho it applies to: Providers or prospective providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Real-world testing may only take place after a testing plan has been drawn up and submitted to the market surveillance authority, and that authority has not objected within 30 days.\nPara 2: The provider must register the testing in the EU database.\nPara 3: Real-world testing must be supervised by the provider or prospective provider. Test subjects must have given informed consent.\nPara 4: The testing plan must include: objectives, methodology, participant protections, risk mitigation measures, monitoring arrangements, and clear criteria for when testing must be suspended or terminated.\nPara 5: Results must be made available to the market surveillance authority upon request.\nPara 6: The provider bears full liability for any harm caused during testing.\nPara 7: Testing must be immediately suspended where safety risks materialise.\nPara 8: Additional safeguards apply for testing involving law enforcement, immigration, or asylum.","61":"What this article does: Establishes the requirements for obtaining informed consent from participants in real-world testing.\n\nWho it applies to: Providers conducting real-world testing and test subjects.\n\nKey requirements:\nPara 1: Informed consent must be freely given, specific, unambiguous, and documented. Participants must be informed of the nature and purpose of the test, the conditions, their right to withdraw, and how to request a reversal or disregard of AI output.\nPara 2: Must be given in clear, simple language. Participants can withdraw at any time without giving reasons or suffering disadvantage.\nPara 3: Consent is without prejudice to GDPR requirements.","62":"What this article does: Requires measures to support SMEs (including start-ups) in complying with the Regulation.\n\nWho it applies to: Member States, the Commission, the AI Office, and SMEs/start-ups.\n\nKey requirements:\nPara 1: Member States must provide SMEs and start-ups with priority access to AI regulatory sandboxes, provided they meet the eligibility conditions.\nPara 2: Must take into account the interests and needs of SME providers and start-ups when setting fees for conformity assessment.\nPara 3: The AI Office must develop templates for the areas covered by this Regulation.\nPara 4: The Commission must establish a dedicated communication channel with SMEs.\nPara 5: Member States must facilitate the participation of start-ups in the standardisation process.","63":"What this article does: Provides derogations for micro and small enterprises from certain procedural requirements.\n\nWho it applies to: Micro-enterprises and small enterprises.\n\nKey requirements:\nPara 1: Certain elements of the quality management system (Article 17) may be simplified, proportionate to the size of the enterprise.\nPara 2: The Commission must develop guidelines and templates tailored to the needs of micro and small enterprises.","64":"What this article does: Establishes the AI Office within the Commission.\n\nWho it applies to: The European Commission.\n\nKey requirements:\nPara 1: The Commission establishes the AI Office to develop Union expertise and capabilities in the field of AI, and to contribute to the implementation of the Regulation.","65":"What this article does: Establishes the European Artificial Intelligence Board.\n\nWho it applies to: The Commission, Member States, and the AI Office.\n\nKey requirements:\nPara 1: The European Artificial Intelligence Board is established as a body of the Union.\nPara 2: The Board consists of one representative per Member State. The European Data Protection Supervisor is an observer.\nPara 3: A chairperson is elected from among the members.\nPara 4: The Board must adopt its rules of procedure.\nPara 5: The AI Office provides the secretariat.","66":"What this article does: Defines the tasks of the European Artificial Intelligence Board.\n\nWho it applies to: The AI Board and Member States.\n\nKey requirements:\nPara 1: The Board must advise and assist the Commission and Member States on: consistent application of the Regulation, recommendations on implementation, collecting and sharing best practices, contributing to uniform administrative practices, providing opinions on standards and common specifications, and supporting cross-border cooperation.\nPara 2: Must contribute to harmonised criteria for sandbox establishment and risk management guidance.\nPara 3: Must issue opinions and recommendations on any matters related to the implementation of the Regulation.\nPara 4: Must support the coordination of joint activities among market surveillance authorities.","67":"What this article does: Establishes an advisory forum to provide technical expertise and stakeholder perspectives to the AI Board and the Commission.\n\nWho it applies to: The Commission, AI Board, and selected stakeholders.\n\nKey requirements:\nPara 1: An advisory forum is established to provide the Board and the Commission with technical expertise and advice.\nPara 2: Members shall represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, academia, and relevant Union bodies.\nPara 3: The Commission shall determine the composition and appoint members for a renewable term of two years.\nPara 4: The forum must establish its rules of procedure and elect a chairperson.","68":"What this article does: Establishes a scientific panel of independent experts to support enforcement, particularly for GPAI models.\n\nWho it applies to: The AI Office and independent scientific experts.\n\nKey requirements:\nPara 1: A scientific panel of independent experts is established to support the AI Office.\nPara 2: Must provide advice on: GPAI model classification as systemic risk, development of codes of practice, guidelines for evaluating GPAI capabilities, technical tools for enforcement, and alerts on systemic risks.\nPara 3: Members must have relevant scientific expertise, be independent, and not have conflicts of interest.\nPara 4: The Commission establishes the composition and operational rules.","69":"What this article does: Allows Member States to access the pool of independent experts for national-level support.\n\nWho it applies to: Member States and their competent authorities.\n\nKey requirements:\nPara 1: Member States may call upon the scientific panel's experts to support their enforcement activities.\nPara 2: Member States may be required to pay fees for the advice provided, which shall be reasonable and proportionate.","70":"What this article does: Requires Member States to designate national competent authorities and a single point of contact for the EU AI Act.\n\nWho it applies to: Member States.\n\nKey requirements:\nPara 1: Each Member State must designate at least one notifying authority and at least one market surveillance authority as national competent authorities.\nPara 2: Must designate a single point of contact for communication with the Commission, the AI Office, other Member States, and the Board.\nPara 3: National competent authorities must be given adequate financial and human resources, and relevant expertise.\nPara 4: Must ensure their independence, impartiality, and absence of conflicts of interest.\nPara 5: Must report annually to the Commission and the Board on the state of AI-related activities.","71":"What this article does: Establishes an EU database for high-risk AI systems listed in Annex III.\n\nWho it applies to: Providers and deployers of Annex III high-risk AI systems, and the Commission.\n\nKey requirements:\nPara 1: The Commission must establish and maintain an EU-wide database, publicly accessible and free of charge.\nPara 2: Providers must register themselves and their systems before placing them on the market.\nPara 3: Deployers that are public authorities must register their use.\nPara 4: The database must contain the information listed in Sections A and B of Annex VIII.\nPara 5: The Commission is responsible for development and maintenance.\nPara 6: Information in the database must be accessible to the public, except for information registered by deployers for law enforcement, migration, asylum, or border control purposes.","72":"What this article does: Requires providers to establish and operate a post-market monitoring system.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must establish and document a post-market monitoring system to actively and systematically collect, document, and analyse data on performance throughout the AI system's lifecycle.\nPara 2: The system must allow the provider to evaluate continuous compliance with the requirements and to identify measures needed.\nPara 3: Must be proportionate to the nature and risk of the AI system.\nPara 4: The post-market monitoring plan must be part of the technical documentation.","73":"What this article does: Requires providers to report serious incidents involving high-risk AI systems.\n\nWho it applies to: Providers of high-risk AI systems.\n\nKey requirements:\nPara 1: Providers must report any serious incident to the market surveillance authority of the Member State where the incident occurred.\nPara 2: Must report immediately after establishing a causal link (or reasonable likelihood thereof) between the AI system and the incident, and no later than 15 days after becoming aware of the serious incident.\nPara 3: The report must contain: the type of AI system, the nature and consequences of the incident, corrective measures taken or envisaged, and any other relevant information.\nPara 4: For widespread incidents affecting multiple Member States, the report should go to the authority where the incident was first reported.\nPara 5: After initial reporting, the provider must promptly report further investigation results and corrective actions.\nPara 6: The market surveillance authority must immediately inform national authorities of other Member States affected.\nPara 7: 'Serious incident' means any incident or malfunctioning that directly or indirectly leads to death, serious damage to health, property, or the environment, or a serious and irreversible disruption in the management of critical infrastructure.","74":"What this article does: Establishes the general framework for market surveillance and control of AI systems in the EU market.\n\nWho it applies to: Market surveillance authorities and all economic operators.\n\nKey requirements:\nPara 1: Market surveillance of AI systems is subject to the existing framework in Regulation (EU) 2019/1020, except where this Regulation provides otherwise.\nPara 2: Market surveillance authorities must have all necessary powers under this Regulation and Regulation (EU) 2019/1020, including the power to access source code, where strictly necessary.\nPara 3: Market surveillance authorities must cooperate with other relevant authorities, including data protection authorities, consumer protection authorities, and the AI Office.\nPara 4-8: Sets rules on confidentiality, testing on premises, cross-border cooperation, and access to documentation.\nPara 9: Must report annually to the Commission.\nPara 10-13: Specific rules for AI used by EU institutions and law enforcement.","75":"What this article does: Establishes mutual assistance and coordination for market surveillance and enforcement of GPAI rules.\n\nWho it applies to: Market surveillance authorities, the AI Office, and the Commission.\n\nKey requirements:\nPara 1: Market surveillance authorities and the AI Office must cooperate, including by sharing information and requesting mutual assistance.\nPara 2: The AI Office is responsible for enforcement of GPAI model obligations (Articles 51-55).\nPara 3: Where a market surveillance authority identifies an issue relating to a GPAI model, it must refer the matter to the AI Office.\nPara 4: The AI Office may request assistance from market surveillance authorities.","76":"What this article does: Sets rules for market surveillance authorities to supervise real-world testing of AI systems.\n\nWho it applies to: Market surveillance authorities and providers conducting real-world tests.\n\nKey requirements:\nPara 1: Market surveillance authorities must have the competence and powers to supervise real-world testing.\nPara 2: Must be notified of all real-world testing in their territory.\nPara 3: May require additional information or impose conditions on the testing.\nPara 4: May order the testing to be suspended or terminated where necessary to protect health, safety, or fundamental rights.","77":"What this article does: Grants specific powers to national authorities responsible for protecting fundamental rights.\n\nWho it applies to: National authorities with fundamental rights mandates (e.g., equality bodies, data protection authorities).\n\nKey requirements:\nPara 1: National authorities may request and access any documentation created or maintained by the deployer of a high-risk AI system, where access is necessary to exercise their powers under national or EU law.\nPara 2: Must inform the market surveillance authority of any such request.\nPara 3: Must use the information only for the exercise of their specific mandate.","78":"What this article does: Sets confidentiality obligations for all parties handling information under the Regulation.\n\nWho it applies to: All competent authorities, notified bodies, and any person handling information under the Regulation.\n\nKey requirements:\nPara 1: Information obtained must be treated as confidential and may only be used for the purposes of the Regulation.\nPara 2: Must protect intellectual property rights, trade secrets, and confidential business information.\nPara 3: This is without prejudice to the rights of the Commission and Member States to exchange information and to issue warnings.\nPara 4: Specific rules on handling personal data in accordance with GDPR.","79":"What this article does: Establishes the national-level procedure for dealing with AI systems that present a risk.\n\nWho it applies to: Market surveillance authorities and economic operators.\n\nKey requirements:\nPara 1: Where a market surveillance authority has sufficient reason to consider that an AI system presents a risk to health, safety, or fundamental rights, it must carry out an evaluation of the system.\nPara 2: Where the authority finds non-compliance, it must require the provider to take corrective action within a reasonable period.\nPara 3: If the provider does not take adequate corrective action, the authority must require withdrawal, prohibition, or restriction of the system.\nPara 4: Must immediately inform the Commission and other Member States.\nPara 5: The procedure must include appropriate measures for systems that present a risk even though they comply with this Regulation.","80":"What this article does: Establishes the procedure when a provider claims its system is not high-risk but an authority disagrees.\n\nWho it applies to: Market surveillance authorities and providers of AI systems.\n\nKey requirements:\nPara 1: Where a market surveillance authority considers that a system classified by the provider as non-high-risk is in fact high-risk, it must require the provider to take corrective action.\nPara 2: The provider must comply with high-risk requirements within the timeframe set by the authority.\nPara 3: If the provider does not comply, the authority may restrict or withdraw the system from the market.\nPara 4: Must inform the Commission and other Member States.","81":"What this article does: Establishes the EU-level safeguard procedure when national measures are disputed.\n\nWho it applies to: The Commission, Member States, and market surveillance authorities.\n\nKey requirements:\nPara 1: Where a Member State raises objections to another Member State's enforcement measure, or the Commission considers a measure contrary to EU law, the Commission must assess the national measure.\nPara 2: The Commission must decide whether the measure is justified within 9 months.\nPara 3: If justified, all Member States must take equivalent measures. If unjustified, the Member State must withdraw the measure.","82":"What this article does: Addresses AI systems that comply with the Regulation but still present a risk.\n\nWho it applies to: Market surveillance authorities and providers.\n\nKey requirements:\nPara 1: Where a market surveillance authority finds a compliant high-risk AI system presents a risk to health, safety, or fundamental rights, it must require the provider to take all appropriate measures to ensure the system no longer presents a risk.\nPara 2: The provider must take corrective action within a specified period.\nPara 3: The authority must immediately inform the Commission and other Member States.","83":"What this article does: Addresses formal non-compliance (technical deficiencies in documentation, marking, etc.) even where no substantive risk exists.\n\nWho it applies to: Market surveillance authorities and providers.\n\nKey requirements:\nPara 1: Where non-compliance is limited to formal aspects (CE marking not affixed, declaration of conformity not drawn up, documentation not available, etc.), the authority must require the provider to rectify the non-compliance.\nPara 2: If the provider does not rectify, the authority must restrict, prohibit, or withdraw the system from the market.","84":"What this article does: Establishes Union AI testing support structures.\n\nWho it applies to: The Commission and Member States.\n\nKey requirements:\nPara 1: The Commission must designate or establish Union AI testing support structures to provide independent technical or scientific advice and support to competent authorities and the AI Office.","85":"What this article does: Grants individuals the right to lodge complaints with market surveillance authorities about AI systems.\n\nWho it applies to: Natural and legal persons, and market surveillance authorities.\n\nKey requirements:\nPara 1: Any natural or legal person having grounds to consider that an AI system has been placed on the market or put into service in infringement of this Regulation may lodge a complaint with the relevant market surveillance authority.\nPara 2: The authority must handle the complaint in accordance with its national procedures.","86":"What this article does: Grants individuals affected by AI decisions the right to receive an explanation.\n\nWho it applies to: Deployers of high-risk AI systems and affected persons.\n\nKey requirements:\nPara 1: Any affected person subject to a decision taken by a deployer based upon the output of a high-risk AI system listed in Annex III (excluding critical infrastructure systems) has the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.\nPara 2: This article does not apply to the use of AI systems for which exceptions from or restrictions to the transparency obligation apply under EU or national law.\nPara 3: This right is without prejudice to other rights under EU and national law, including the right to an effective remedy and to a fair trial.","87":"What this article does: Protects whistleblowers who report infringements of the Regulation.\n\nWho it applies to: Any person reporting infringements and the authorities receiving reports.\n\nKey requirements:\nPara 1: The Whistleblower Protection Directive (EU) 2019/1937 applies to the reporting of infringements of this Regulation.","88":"What this article does: Sets out the enforcement mechanism for GPAI model obligations.\n\nWho it applies to: The AI Office and providers of GPAI models.\n\nKey requirements:\nPara 1: The AI Office is responsible for the supervision and enforcement of GPAI model obligations (Articles 51-55).\nPara 2: The AI Office must cooperate with national competent authorities.","89":"What this article does: Empowers the AI Office to conduct monitoring actions on GPAI model providers.\n\nWho it applies to: The AI Office and GPAI model providers.\n\nKey requirements:\nPara 1: The AI Office may take necessary actions to monitor GPAI model providers' compliance with this Regulation.\nPara 2: Downstream providers may lodge complaints about infringements.\nPara 3: The AI Office must have due regard for confidentiality and trade secrets.","90":"What this article does: Enables the scientific panel to alert the AI Office to potential systemic risks from GPAI models.\n\nWho it applies to: The scientific panel of independent experts and the AI Office.\n\nKey requirements:\nPara 1: The scientific panel may issue a qualified alert to the AI Office where it has reason to suspect that a GPAI model poses a concrete, identifiable risk at Union level.\nPara 2: The AI Office may, following such alert, carry out an evaluation of the model.\nPara 3: The AI Office must inform the Board of any alert and measures taken.","91":"What this article does: Grants the AI Office power to request documentation and information from GPAI model providers.\n\nWho it applies to: The AI Office and GPAI model providers.\n\nKey requirements:\nPara 1: The AI Office may request GPAI model providers to provide documentation and information necessary to assess compliance, including access to the model and its documentation.\nPara 2: Providers must cooperate with the AI Office and provide the requested information within a reasonable time period.\nPara 3: The AI Office may conduct on-site inspections of providers' premises.","92":"What this article does: Gives the AI Office the power to conduct evaluations of GPAI models.\n\nWho it applies to: The AI Office and GPAI model providers.\n\nKey requirements:\nPara 1: The AI Office may conduct evaluations of GPAI models to assess compliance, including by requesting the provider to carry out specific tests.\nPara 2: May request access to the model through APIs or other appropriate technical means.\nPara 3: May use independent experts or external organisations to assist in evaluations.\nPara 4: Providers must cooperate with the AI Office during evaluations.\nPara 5: The AI Office must protect trade secrets and confidential information.","93":"What this article does: Grants the AI Office the power to request that GPAI model providers take corrective measures.\n\nWho it applies to: The AI Office and GPAI model providers.\n\nKey requirements:\nPara 1: Where the AI Office identifies non-compliance, it may request the provider to take appropriate measures to come into compliance.\nPara 2: May also request the provider to restrict the model's availability, withdraw it from the market, or recall it.\nPara 3: The provider must comply or provide reasons why the identified risks cannot be remedied.","94":"What this article does: Protects the procedural rights of economic operators during GPAI enforcement.\n\nWho it applies to: Economic operators (GPAI model providers) subject to enforcement.\n\nKey requirements:\nPara 1: Article 18 of Regulation (EU) 2019/1020 (right to be heard, access to file, etc.) applies to the GPAI enforcement procedures under this Regulation.","95":"What this article does: Encourages the development of voluntary codes of conduct for non-high-risk AI systems.\n\nWho it applies to: All AI providers and deployers (voluntary).\n\nKey requirements:\nPara 1: The AI Office and Member States must encourage and facilitate the drawing up of voluntary codes of conduct for AI systems not classified as high-risk, covering some or all of the requirements for high-risk AI on a voluntary basis.\nPara 2: Codes may also address environmental sustainability, accessibility for persons with disabilities, stakeholder participation, and diversity of development teams.\nPara 3: The Commission and AI Board may encourage adoption of codes.\nPara 4: Codes must include clear objectives, indicators, and governance mechanisms.","96":"What this article does: Requires the Commission to issue guidelines on the implementation of the Regulation.\n\nWho it applies to: The Commission.\n\nKey requirements:\nPara 1: The Commission must issue guidelines on the practical implementation of the Regulation, covering: the application of the requirements in Articles 8-15, the prohibited practices in Article 5, the classification of systems under Article 6, the calculation of fines, and the application of the right to explanation.\nPara 2: Must consult the Board, Member States, and relevant stakeholders.\nPara 3: Guidelines must be reviewed regularly.","97":"What this article does: Sets the procedural rules for the Commission's exercise of delegated powers.\n\nWho it applies to: The Commission, European Parliament, and Council.\n\nKey requirements:\nPara 1: The power to adopt delegated acts is conferred on the Commission subject to the conditions in this article.\nPara 2: Delegation of power is conferred for an indeterminate period from the entry into force.\nPara 3: Delegation may be revoked at any time by the European Parliament or the Council.\nPara 4: Delegated acts enter into force only if no objection has been expressed by the Parliament or Council within 3 months.","98":"What this article does: Establishes the committee procedure for the Commission's implementing acts.\n\nWho it applies to: The Commission and the committee of Member State representatives.\n\nKey requirements:\nPara 1: The Commission is assisted by a committee within the meaning of Regulation (EU) No 182/2011.\nPara 2: The examination procedure applies.","99":"What this article does: Sets the penalty framework for infringements of the Regulation.\n\nWho it applies to: Member States and all economic operators subject to the Regulation.\n\nKey requirements:\nPara 1: Member States must lay down rules on penalties (including administrative fines) and take all measures necessary to ensure proper implementation.\nPara 2: For prohibited AI practices (Article 5): fines up to €35 million or 7% of total worldwide annual turnover (whichever is higher).\nPara 3: For non-compliance with obligations other than Article 5: fines up to €15 million or 3% of total worldwide annual turnover.\nPara 4: For supplying incorrect, incomplete, or misleading information to authorities or notified bodies: fines up to €7.5 million or 1% of total worldwide annual turnover.\nPara 5: For SMEs (including start-ups), the amounts are the lower of the percentage or the absolute amount. For other companies, the higher of the two.\nPara 6: Penalties must be effective, proportionate, and dissuasive, taking into account: the nature and gravity of the infringement, whether it was intentional or negligent, actions taken to mitigate harm, the size and market share of the infringer, previous infringements, the degree of cooperation, and any other relevant factors.\nPara 7: Each Member State must notify the Commission of its penalty rules.","100":"What this article does: Extends the administrative fines regime to EU institutions, bodies, offices, and agencies.\n\nWho it applies to: EU institutions acting as providers or deployers of AI systems.\n\nKey requirements:\nPara 1: The European Data Protection Supervisor (EDPS) may impose administrative fines on EU institutions for infringements of this Regulation.\nPara 2: The fine thresholds mirror Article 99: up to €1.5 million for prohibited practices, up to €750,000 for other obligations, and up to €375,000 for misleading information.\nPara 3: The EDPS must take into account the same factors as in Article 99(6).","101":"What this article does: Sets the fines framework specifically for providers of GPAI models.\n\nWho it applies to: The Commission (AI Office) and GPAI model providers.\n\nKey requirements:\nPara 1: The Commission may fine GPAI model providers up to €15 million or 3% of total worldwide annual turnover for infringements of GPAI obligations (Articles 51-55).\nPara 2: For supplying incorrect or misleading information in response to AI Office requests: fines up to €7.5 million or 1% of worldwide annual turnover.\nPara 3: Must consider the same proportionality factors as Article 99(6).\nPara 4: Fines imposed on SMEs must be proportionate.","102":"What this article does: Amends Regulation (EC) No 300/2008 on common rules for civil aviation security to incorporate AI Act references.\n\nWho it applies to: Entities subject to aviation security regulation.\n\nKey requirements:\nThe amendment ensures that where AI systems are used as safety components in the aviation security context, the AI Act requirements apply through the existing conformity assessment framework.","103":"What this article does: Amends Regulation (EU) No 167/2013 on agricultural and forestry vehicles to incorporate AI Act references.\n\nWho it applies to: Manufacturers of agricultural and forestry vehicles using AI components.\n\nKey requirements:\nEnsures AI safety components in agricultural/forestry vehicles are subject to AI Act requirements through the existing type-approval framework.","104":"What this article does: Amends Regulation (EU) No 168/2013 on two- or three-wheeled vehicles to incorporate AI Act references.\n\nWho it applies to: Manufacturers of two/three-wheeled vehicles using AI components.\n\nKey requirements:\nEnsures AI safety components in these vehicles are subject to AI Act requirements through the existing type-approval framework.","105":"What this article does: Amends Directive 2014/90/EU on marine equipment to incorporate AI Act references.\n\nWho it applies to: Manufacturers of marine equipment using AI components.\n\nKey requirements:\nEnsures AI safety components in marine equipment are subject to AI Act requirements through the existing conformity assessment framework.","106":"What this article does: Amends Directive (EU) 2016/797 on the interoperability of the rail system to incorporate AI Act references.\n\nWho it applies to: Operators and manufacturers in the railway sector using AI components.\n\nKey requirements:\nEnsures AI safety components in rail systems are subject to AI Act requirements through the existing conformity assessment framework.","107":"What this article does: Amends Regulation (EU) 2018/858 on motor vehicle type-approval to incorporate AI Act references.\n\nWho it applies to: Motor vehicle manufacturers using AI components.\n\nKey requirements:\nEnsures AI safety components in motor vehicles are subject to AI Act requirements through the existing type-approval framework.","108":"What this article does: Amends Regulation (EU) 2018/1139 on common rules in civil aviation to incorporate AI Act references.\n\nWho it applies to: Aviation industry participants using AI components.\n\nKey requirements:\nEnsures AI safety components in civil aviation are subject to AI Act requirements through EASA's existing framework. Specific provisions for the role of EASA as a notified body for aviation AI systems.","109":"What this article does: Amends Regulation (EU) 2019/2144 on motor vehicle type-approval (safety) to incorporate AI Act references.\n\nWho it applies to: Motor vehicle manufacturers using AI safety systems.\n\nKey requirements:\nEnsures AI safety components in motor vehicle safety systems are subject to AI Act requirements through the existing type-approval framework.","110":"What this article does: Amends Directive (EU) 2020/1828 on representative actions to include the EU AI Act.\n\nWho it applies to: Consumer organisations and affected persons.\n\nKey requirements:\nAdds the EU AI Act to the list of EU laws that can be enforced through representative actions, enabling consumer organisations to bring collective actions for infringements of this Regulation.","111":"What this article does: Sets transitional provisions for AI systems already on the market and GPAI models already in use.\n\nWho it applies to: Providers and deployers of AI systems already placed on the market before the Regulation's application dates.\n\nKey requirements:\nPara 1: The Regulation does not apply to AI systems placed on the market or put into service before the applicable dates, unless they are subsequently significantly modified.\nPara 2: Providers of high-risk AI systems placed on the market before the application dates must comply with Article 16 obligations by the relevant deadline only if those systems undergo significant modifications in design.\nPara 3: Providers of GPAI models placed on the market before the GPAI application date must comply with Articles 53 and 55 within the applicable deadline.","112":"What this article does: Requires the Commission to evaluate and review the Regulation periodically.\n\nWho it applies to: The Commission, with support from the Board and other bodies.\n\nKey requirements:\nPara 1: The Commission must evaluate the need for amendments annually, including to the list of high-risk AI systems in Annex III.\nPara 2: Must assess the need to amend Annex I (product safety legislation), Annex III categories, and the penalty framework.\nPara 3: Must submit a report to the European Parliament and Council by specified deadlines covering: the state of resources of national competent authorities, the state of penalties across Member States, the development of standards and harmonised approaches, and the impact on SMEs and start-ups.\nPara 4: Must assess the effectiveness and governance of the AI Office.\nPara 5: Must assess the need for additional legislation on liability for AI.\nPara 6-10: Specific review timelines for various aspects including GPAI classification thresholds, the list of prohibited practices, and the enforcement framework.","113":"What this article does: Sets out the staggered timeline for when different parts of the Regulation enter into force and become applicable.\n\nWho it applies to: All persons subject to the Regulation.\n\nKey requirements:\nPara 1: The Regulation enters into force 20 days after publication in the Official Journal (1 August 2024).\nPara 2: Application dates are staggered:\n- 2 February 2025: Chapter I (General Provisions) and Chapter II (Prohibited Practices, Article 5)\n- 2 August 2025: Chapter III Section 4 (Notified Bodies), Chapter V (GPAI Models), Chapter VII (Governance), Articles 78-84 (Confidentiality and Market Surveillance), and Article 99 (Penalties)\n- 2 August 2026: All remaining provisions, including the high-risk AI requirements in Chapter III Sections 1-3, deployer obligations, conformity assessment, and the EU database\n- 2 August 2027: Article 6(1) — high-risk classification for AI in products subject to Annex I product safety legislation\n\nKey exceptions: Certain articles relating to specific product sectors have different application dates. Articles 103-109 (sectoral amendments) apply from 2 August 2027."};
